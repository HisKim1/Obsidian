**작업**
파일명: `/home/hiskim1/graphcast/graphcast_demo.ipynb` 14번째 cell
작업 `conda` 환경: `hiskim1_graphcast /home/hiskim1/.conda/envs/hiskim1_graphcast`[^1]

**문제 상황**
코드를 실행하려면 *cuDNN library*  8.6 버전 이상이 필요함
& 해당 *cuDNN library*에서 구동 가능한 *[jax library](https://jax.readthedocs.io/en/latest/index.html)* 가 필요함 

**요청 사항**
1. *cuDNN library* 를 기존 8.7.0으로 롤백 혹은 8.6으로[^2]
2.  해당 *cuDNN library*과 호환되는 *jax library* 설치

---
**14th cell**
```python
# @title Build jitted functions, and possibly initialize random weights

# This function initializes and wraps the GraphCast predictor, applying BFloat16 casting, normalization, and autoregressive wrapping.
def construct_wrapped_graphcast(
    model_config: graphcast.ModelConfig,
    task_config: graphcast.TaskConfig):
  """Constructs and wraps the GraphCast Predictor."""
  # Deeper one-step predictor.
  # Initialize the GraphCast predictor with the provided model and task configurations.
  predictor = graphcast.GraphCast(model_config, task_config)
    # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to
  # from/to float32 to/from BFloat16.
  # Wrap the predictor to handle conversion between float32 and BFloat16.
  predictor = casting.Bfloat16Cast(predictor)
  # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from
  # BFloat16 happens after applying normalization to the inputs/targets.
  # Wrap the predictor to apply normalization to inputs/targets before casting to/from BFloat16.
  predictor = normalization.InputsAndResiduals(
      predictor,
      diffs_stddev_by_level=diffs_stddev_by_level,
      mean_by_level=mean_by_level,
      stddev_by_level=stddev_by_level)

  # Wraps everything so the one-step model can produce trajectories.
  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)
  return predictor

# This function defines the forward pass of the model using the constructed predictor.
@hk.transform_with_state
def run_forward(model_config, task_config, inputs, targets_template, forcings):
    """Runs the forward pass using the constructed predictor."""
    predictor = construct_wrapped_graphcast(model_config, task_config)
    return predictor(inputs, targets_template=targets_template, forcings=forcings)

# This function calculates the loss and diagnostics for the given inputs, targets, and forcings.
@hk.transform_with_state
def loss_fn(model_config, task_config, inputs, targets, forcings):
    """Calculates the loss and diagnostics using the constructed predictor."""
    predictor = construct_wrapped_graphcast(model_config, task_config)
    loss, diagnostics = predictor.loss(inputs, targets, forcings)
    return xarray_tree.map_structure(
        lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),
        (loss, diagnostics)
    )

# This function calculates the gradients of the loss function with respect to the parameters.
def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):
  def _aux(params, state, i, t, f):
    (loss, diagnostics), next_state = loss_fn.apply(
        params, state, jax.random.PRNGKey(0), model_config, task_config,
        i, t, f)
    return loss, (diagnostics, next_state)
  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(
      _aux, has_aux=True)(params, state, inputs, targets, forcings)
  return loss, diagnostics, next_state, grads

# Jax doesn't seem to like passing configs as args through the jit. Passing it
# in via partial (instead of capture by closure) forces jax to invalidate the
# jit cache if you change configs.
# Utility function to apply model and task configurations through functools.partial
def with_configs(fn):
  return functools.partial(
      fn, model_config=model_config, task_config=task_config)

# Always pass params and state, so the usage below are simpler
# Ensures functions always receive parameters and state.
def with_params(fn):
  return functools.partial(fn, params=params, state=state)

# Our models aren't stateful, so the state is always empty, so just return the
# predictions. This is requiredy by our rollout code, and generally simpler.
def drop_state(fn):
  return lambda **kw: fn(**kw)[0]
  
# JIT compile the initialization of the forward pass
init_jitted = jax.jit(with_configs(run_forward.init))
  
if params is None:
    # Initialize parameters and state if they are not already set
    params, state = init_jitted(
        rng=jax.random.PRNGKey(0),
        inputs=train_inputs,
        targets_template=train_targets,
        forcings=train_forcings
    )
# JIT compile the loss function, gradients function, and forward pass with necessary configurations and parameters
loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))
grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))
run_forward_jitted = drop_state(with_params(jax.jit(with_configs(run_forward.apply))))
```

**14th cell's Output**
```
2024-07-05 09:38:33.462094: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:449] 
Loaded runtime CuDNN library: 8.2.1 but source was compiled with: 8.6.0.  
CuDNN library needs to have matching major version and equal or higher minor version. 
If using a binary install, upgrade your CuDNN library.  
If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
```

**Error Message**
```python
XlaRuntimeError                           Traceback (most recent call last)
Cell In[14], line 84
     79 init_jitted = jax.jit(with_configs(run_forward.init))
     81 if params is None:
     82     # Initialize parameters and state if they are not already set
     83     params, state = init_jitted(
---> 84         rng=jax.random.PRNGKey(0),
     85         inputs=train_inputs,
     86         targets_template=train_targets,
     87         forcings=train_forcings
     88     )
     90 # JIT compile the loss function, gradients function, and forward pass with necessary configurations and parameters
     91 loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))

File ~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/jax/_src/random.py:224, in PRNGKey(seed, impl)
    207 def PRNGKey(seed: int | ArrayLike, *,
    208             impl: PRNGSpecDesc | None = None) -> KeyArray:
    209   """Create a pseudo-random number generator (PRNG) key given an integer seed.
    210 
    211   The resulting key carries the default PRNG implementation, as
   (...)
    222     and ``fold_in``.
    223   """
--> 224   return _return_prng_keys(True, _key('PRNGKey', seed, impl))
...
    254 # TODO(sharadmv): remove this fallback when all backends allow `compile`
    255 # to take in `host_callbacks`
--> 256 return backend.compile(built_c, compile_options=options)

XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```

---
[^1]: 지난 번에 보내주신 대로 *jax*는 0.4.23으로 설치되었고 GPU 인식도 잘 됩니다.
[^2]: 8.6 상위버전들이 8.6.0에서 쓰인 이 코드와 호환이 되는지 잘 모르겠습니다. 가능하다면 최신 버전으로 해두는 게 이후 사용자들에게 좋지 않을까 싶습니다.
